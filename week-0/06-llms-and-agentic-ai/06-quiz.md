# Lesson: LLMs and Agentic AI

### Multiple Choice Questions

**Question 1:**  
What fundamental shift does the transformer architecture represent compared to traditional models like RNNs?

a) Moving from parallel to sequential processing  
b) Moving from sequential to parallel processing  
c) Moving from supervised to unsupervised learning  
d) Moving from symbolic to neural networks  


**Question 2:**  
Using the formula for calculating an LLM's memory footprint, approximately how much memory would a 13B parameter model in FP16 precision require just for storing weights?

a) 13GB  
b) 26GB  
c) 52GB  
d) 104GB  


**Question 3:**  
What is the primary benefit of domain-specific LLMs like BloombergGPT compared to general-purpose models?

a) They require less computational resources  
b) They can understand multiple languages better  
c) They demonstrate superior performance on domain-specific tasks  
d) They generate text faster than general-purpose models  


**Question 4:**  
Which of the following is NOT identified as a key component of effective prompts?

a) Clear instructions  
b) Relevant context  
c) Format specification  
d) Error handling  


**Question 5:**  
What is the primary purpose of Retrieval-Augmented Generation (RAG) in LLM applications?

a) To increase generation speed  
b) To reduce model size  
c) To ground responses in external knowledge sources  
d) To improve prompt engineering techniques  


### Multiple Answer Questions

**Question 6:**  
Which of the following are true about self-attention mechanisms in transformer models? (Select all that apply)

a) They compute relevance between all token pairs directly  
b) They process tokens sequentially like a linked list  
c) They enable capturing relationships regardless of distance  
d) They create a computational complexity of O(nÂ²) with sequence length  


**Question 7:**  
Which optimization techniques can significantly improve LLM performance and reduce resource requirements? (Select all that apply)

a) Quantization  
b) Pruning  
c) Distillation  
d) Increasing model parameters  


**Question 8:**  
Which deployment architectures are discussed for LLM implementation? (Select all that apply)

a) Dedicated inference servers  
b) Serverless inference  
c) Edge deployment  
d) Hybrid architectures  


**Question 9:**  
Which advanced prompting techniques are described to enhance LLM performance? (Select all that apply)

a) Few-shot learning  
b) Chain-of-thought prompting  
c) Structured output formatting  
d) Diffusion-based generation  


**Question 10:**  
Which approaches are emerging for improving LLM evaluation and explainability? (Select all that apply)

a) Attention visualization  
b) Confidence scoring  
c) Contrastive explanations  
d) Randomized verification  
