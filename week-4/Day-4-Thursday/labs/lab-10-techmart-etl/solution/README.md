# TechMart ETL Pipeline

A comprehensive ETL (Extract, Transform, Load) pipeline for TechMart's multi-source data integration, featuring star schema data warehouse design, data quality monitoring, and automated testing.

## ğŸ“‹ Project Overview

The TechMart ETL Pipeline integrates data from four different sources:
- **CSV Files**: Daily sales transactions with quality issues
- **JSON Files**: Product catalog with schema variations  
- **MongoDB**: Customer profiles and preferences
- **SQL Server**: Customer service support tickets

Data is transformed using **Slowly Changing Dimensions (SCD Type 1)** and loaded into a **star schema data warehouse** with comprehensive monitoring and quality scoring.

## ğŸ“Š Source Data Files

### Standalone Source Files (Not Associated with Database Servers)
The following files are direct file sources that require no database setup:

1. **CSV Source**: `student_distribution/Source CSV/daily_sales_transactions.csv`
   - **Type**: File-based source
   - **Content**: Sales transaction data from legacy POS system
   - **Records**: 1,000 transactions
   - **Purpose**: Transaction data with intentional quality issues for ETL processing
   - **Access**: Direct file reading (pandas, CSV libraries)

2. **JSON Source**: `student_distribution/Source JSON/product_catalog.json`
   - **Type**: File-based source  
   - **Content**: Product catalog from modern inventory management API
   - **Records**: 500+ products (4,000+ lines of JSON)
   - **Purpose**: Product data with schema variations for ETL processing
   - **Access**: Direct file reading (json libraries)

### Database Source Files (Require Database Setup)
The following require running setup scripts to create database sources:

3. **SQL Server Sources**: `student_distribution/SQL Server/`
   - **Files**: customers.csv, support_tickets.csv, ticket_categories.csv
   - **Setup**: Run `setup_sql_server.py` to create database and import data
   - **Database**: techmart_customer_service
   - **Purpose**: Customer service tickets and customer data

4. **MongoDB Sources**: `student_distribution/MongoDB Server/`
   - **Files**: customer_profiles.json
   - **Setup**: Run `setup_mongodb.py` to create database and import data
   - **Database**: techmart_customers
   - **Purpose**: Customer profiles and demographics

### File Locations Summary
- **Standalone CSV/JSON**: Located in `student_distribution/Source CSV/` and `student_distribution/Source JSON/`
- **Database Data**: Located in `student_distribution/SQL Server/` and `student_distribution/MongoDB Server/`
- **Setup Scripts**: Included in respective database folders to create source databases

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚    â”‚   ETL Pipeline   â”‚    â”‚ Data Warehouse  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ CSV Files     â”‚â”€â”€â”€â–¶â”‚ â€¢ Extract        â”‚â”€â”€â”€â–¶â”‚ â€¢ Fact Tables   â”‚
â”‚ â€¢ JSON Files    â”‚    â”‚ â€¢ Transform      â”‚    â”‚ â€¢ Dimension     â”‚
â”‚ â€¢ MongoDB       â”‚    â”‚ â€¢ Load           â”‚    â”‚   Tables        â”‚
â”‚ â€¢ SQL Server    â”‚    â”‚ â€¢ Quality Check  â”‚    â”‚ â€¢ SCD Type 1    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Monitoring     â”‚
                    â”‚ â€¢ Quality Score  â”‚
                    â”‚ â€¢ Power BI       â”‚
                    â”‚ â€¢ Metrics        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- SQL Server (with appropriate permissions)
- MongoDB (for source data)
- Git

### Installation

1. **Clone and Setup Environment**
```bash
git clone <repository-url>
cd techmart-etl-pipeline
pip install -r requirements.txt
```

2. **Database Setup**
```bash
# Set up databases and create schemas
python setup_databases.py

# Import master datasets (if available)
python data_generators/student_data_import.py
```

3. **Generate Sample Data** (if no master datasets)
```bash
python data_generators/csv_data_generator.py
python data_generators/json_data_generator.py
python data_generators/mongodb_data_generator.py
python data_generators/sql_server_data_generator.py
```

4. **Run ETL Pipeline**
```bash
python src/etl_pipeline.py
```

5. **Run Tests**
```bash
pytest -v  # All tests
pytest tests/test_unit.py -v  # Unit tests only
pytest tests/test_integration.py -v  # Integration tests
pytest tests/test_e2e.py -v  # End-to-end test
```

## ğŸ“ Project Structure

```
techmart-etl-pipeline/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ config.py                          # Configuration settings
â”œâ”€â”€ conftest.py                        # Global test configuration
â”œâ”€â”€ setup_databases.py                 # Database setup script
â”œâ”€â”€ data_warehouse_schema.sql          # Star schema DDL
â”‚
â”œâ”€â”€ src/                               # Core ETL Application
â”‚   â”œâ”€â”€ __init__.py                    # Package initialization
â”‚   â”œâ”€â”€ etl_pipeline.py               # Main pipeline orchestrator
â”‚   â”œâ”€â”€ extractors.py                 # Data extraction logic
â”‚   â”œâ”€â”€ transformers.py               # Data transformation & SCD
â”‚   â”œâ”€â”€ loaders.py                    # Data warehouse loading
â”‚   â”œâ”€â”€ data_quality.py               # Quality validation
â”‚   â””â”€â”€ monitoring.py                 # Metrics and monitoring
â”‚
â”œâ”€â”€ tests/                             # Comprehensive Test Suite
â”‚   â”œâ”€â”€ __init__.py                    # Test package initialization
â”‚   â”œâ”€â”€ conftest.py                    # Test fixtures
â”‚   â”œâ”€â”€ test_unit.py                   # 5 unit tests
â”‚   â”œâ”€â”€ test_integration.py            # 3 integration tests
â”‚   â”œâ”€â”€ test_e2e.py                    # 1 end-to-end test
â”‚   â””â”€â”€ test_fixtures.py               # Test data fixtures
â”‚
â”œâ”€â”€ data_generators/                   # Data Generation Scripts
â”‚   â”œâ”€â”€ csv_data_generator.py          # Sales CSV generator
â”‚   â”œâ”€â”€ json_data_generator.py         # Products JSON generator
â”‚   â”œâ”€â”€ mongodb_data_generator.py      # Customer MongoDB generator
â”‚   â”œâ”€â”€ sql_server_data_generator.py   # Support SQL generator
â”‚   â”œâ”€â”€ create_master_datasets.py      # Master dataset creator
â”‚   â””â”€â”€ student_data_import.py         # Student data importer
â”‚
â”œâ”€â”€ generated_data/                    # Generated/Working Data
â”œâ”€â”€ test_data/                         # Sample test data
â”œâ”€â”€ master_datasets/                   # Master dataset distribution
â”œâ”€â”€ student_distribution/              # Student data packages
â”œâ”€â”€ monitoring/                        # Monitoring outputs
â”œâ”€â”€ docs/                             # Documentation
â”œâ”€â”€ logs/                             # Pipeline execution logs
â””â”€â”€ temp/                             # Temporary processing files
```

## ğŸ”§ Configuration

### Database Connections
Edit `config.py` to configure your database connections:

```python
DATABASE_CONFIG = {
    'sql_server': {
        'server': 'your-server',
        'database': 'TechMartDW',
        'username': 'your-username',
        'password': 'your-password'
    },
    'mongodb': {
        'connection_string': 'mongodb://localhost:27017/',
        'database': 'techmart_customers'
    }
}
```

### Pipeline Settings
Key configuration options:

- **Batch Size**: Number of records processed per batch
- **Quality Threshold**: Minimum data quality score (0-100)
- **SCD Settings**: Slowly Changing Dimension configuration
- **Monitoring**: Enable/disable Power BI metrics export

## ğŸ“Š Data Quality & Monitoring

### Quality Scoring
The pipeline implements comprehensive data quality checks:
- **Completeness**: Missing value detection
- **Validity**: Data type and format validation  
- **Consistency**: Cross-source data alignment
- **Accuracy**: Business rule validation

### Monitoring Dashboard
Metrics are exported for Power BI integration:
- Pipeline execution times
- Data quality scores
- Record counts and processing rates
- Error rates and failure points

## ğŸ§ª Testing Framework

### Test Categories
- **Unit Tests (5)**: Individual component testing
  - Data extraction functions
  - Transformation logic
  - SCD implementation
  - Error handling 
  - Data validation

- **Integration Tests (3)**: Multi-component testing
  - Multi-source data integration
  - End-to-end pipeline flow
  - Database loading operations

- **End-to-End Test (1)**: Complete business process
  - Full pipeline execution with real data scenarios

### Running Tests
```bash
# All tests with coverage
pytest --cov=src tests/

# Specific test categories
pytest tests/test_unit.py -v
pytest tests/test_integration.py -v
pytest tests/test_e2e.py -v

# Test with markers
pytest -m "not slow" -v  # Skip slow tests
pytest -m "quality" -v   # Run only quality tests
```

## ğŸ“š Documentation

Comprehensive documentation available in `docs/`:
- `data_dictionary.md` - Complete data warehouse schema
- `pipeline_monitoring.md` - Monitoring setup guide
- `etl_lab_assignment.md` - Student assignment details
- `instructor_lab_guide.md` - Instructor completion guide
- `testing_supplemental_lesson.md` - Testing implementation

## ğŸ¯ Development Workflow

### For Students
1. Import master datasets using `student_data_import.py`
2. Implement ETL functions in `src/` files (follow TODO comments)
3. Write and run tests for your implementations
4. Execute full pipeline and validate results
5. Generate monitoring reports

### For Instructors
1. Create master datasets using `create_master_datasets.py`
2. Distribute via `student_distribution/` package
3. Use instructor guides for assessment criteria
4. Monitor student progress via test execution

## ğŸš¨ Troubleshooting

### Common Issues

**Database Connection Errors**
```bash
# Check SQL Server connection
python -c "from src.config import test_sql_connection; test_sql_connection()"

# Check MongoDB connection  
python -c "from src.config import test_mongodb_connection; test_mongodb_connection()"
```

**Missing Dependencies**
```bash
pip install -r requirements.txt --upgrade
```

**Data Quality Issues**
```bash
# Run data quality assessment
python src/data_quality.py --assess-all

# Check logs
tail -f logs/data_quality.log
```

**Test Failures**
```bash
# Run with detailed output
pytest -v -s --tb=long

# Run specific failing test
pytest tests/test_unit.py::test_specific_function -v
```

## ğŸ“ˆ Performance Optimization

### Batch Processing
- Default batch size: 1,000 records
- Configurable via `config.py`
- Monitor memory usage for large datasets

### Database Optimization
- Indexed fact and dimension tables
- Optimized SCD queries
- Connection pooling enabled

## ğŸ”’ Security Considerations

- Database credentials stored in environment variables
- Connection string encryption recommended
- Audit logging for all data operations
- Role-based access control implementation

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## ğŸ“„ License

This project is part of the Data Engineering Course curriculum. 
Educational use permitted. Commercial use requires authorization.

## ğŸ“ Support

- **Documentation**: Check `docs/` directory
- **Issues**: Create GitHub issue with detailed description
- **Testing**: Run `pytest --collect-only` to verify test discovery
- **Configuration**: Use `python setup_databases.py --help`

---

**Version**: 1.0.0  
**Last Updated**: 2025-01-06  
**Maintenance**: Active Development
