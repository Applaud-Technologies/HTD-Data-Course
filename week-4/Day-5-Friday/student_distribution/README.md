# BookHaven ETL Assessment

A comprehensive ETL (Extract, Transform, Load) pipeline for BookHaven, integrating data from multiple sources (MongoDB, SQL Server, CSV, JSON) into a star schema data warehouse. Includes advanced data quality validation, cleaning, and modular design for student extension.

## ğŸ“‹ Project Overview

The BookHaven ETL Assessment challenges you to build a robust pipeline that:
- Extracts data from:
  - **MongoDB**: Customer profiles, reading history, genre preferences
  - **SQL Server**: Orders, inventory, customer master
  - **CSV**: Book catalog (with series, genres, recommendations)
  - **JSON**: Author profiles (including collaborations)
- Handles complex relationships: book series, author collaborations, customer reading history, book recommendations, and genre preferences
- Cleans and validates data with advanced rules and severity levels
- Loads data into a **star schema** in SQL Server
- Provides detailed data quality reporting

## ğŸ“Š Source Data Files

- **CSV**: `data/csv/book_catalog.csv` â€” Book catalog with intentional data quality issues
- **JSON**: `data/json/author_profiles.json` â€” Author profiles, including collaborations and issues
- **MongoDB**: `data/mongodb/customers.json` â€” Customer profiles, reading history, preferences
- **SQL Server**: `data/sqlserver/` â€” Orders, inventory, customers (imported via script)

Each data generator script introduces missing values, inconsistent formats, duplicates, and invalid data for realistic ETL challenges.

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚    â”‚   ETL Pipeline   â”‚    â”‚   Star Schema DW   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ CSV Catalog   â”‚â”€â”€â”€â–¶â”‚ â€¢ Extract        â”‚â”€â”€â”€â–¶â”‚ â€¢ Fact Table:      â”‚
â”‚ â€¢ JSON Authors  â”‚    â”‚ â€¢ Transform      â”‚    â”‚   Book Sales       â”‚
â”‚ â€¢ MongoDB Cust  â”‚    â”‚ â€¢ Clean/Validate â”‚    â”‚ â€¢ Dim: Book, Authorâ”‚
â”‚ â€¢ SQL Orders    â”‚    â”‚ â€¢ Load           â”‚    â”‚   Customer, Date   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Monitoring     â”‚
                    â”‚ â€¢ Quality Score  â”‚
                    â”‚ â€¢ Reports        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Docker (for MongoDB and SQL Server)
- Git

### Installation

1. **Clone and Setup Environment**
```bash
git clone <repository-url>
cd bookhaven-etl-assessment
pip install -r requirements.txt
```

2. **Start Databases**
```bash
docker-compose -f docker-compose-mongodb.yml up -d
docker-compose -f docker-compose-sqlserver.yml up -d
```

3. **Generate Sample Data**
```bash
python data_generators/csv_book_catalog_generator.py
python data_generators/json_author_profiles_generator.py
python data_generators/mongodb_customers_generator.py
python data_generators/sqlserver_orders_inventory_generator.py
```

4. **Run ETL Pipeline**
```bash
python etl/etl_pipeline.py
```

5. **Run Tests**
```bash
pytest -v
```

## ğŸ“ Project Structure

```
bookhaven-etl-assessment/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose-mongodb.yml
â”œâ”€â”€ docker-compose-sqlserver.yml
â”œâ”€â”€ config.py
â”‚
â”œâ”€â”€ data_generators/
â”‚   â”œâ”€â”€ csv_book_catalog_generator.py
â”‚   â”œâ”€â”€ json_author_profiles_generator.py
â”‚   â”œâ”€â”€ mongodb_customers_generator.py
â”‚   â””â”€â”€ sqlserver_orders_inventory_generator.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ csv/
â”‚   â”œâ”€â”€ json/
â”‚   â”œâ”€â”€ mongodb/
â”‚   â””â”€â”€ sqlserver/
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extractors.py
â”‚   â”œâ”€â”€ transformers.py
â”‚   â”œâ”€â”€ loaders.py
â”‚   â”œâ”€â”€ data_quality.py
â”‚   â”œâ”€â”€ cleaning.py
â”‚   â”œâ”€â”€ etl_pipeline.py
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_unit.py
â”‚   â”œâ”€â”€ test_integration.py
â”‚   â”œâ”€â”€ test_e2e.py
â”‚   â””â”€â”€ test_fixtures.py
â”‚
â””â”€â”€ docs/
```

## ğŸ”§ Configuration

Edit `etl/config.py` to set database connections and pipeline options:

```python
DATABASE_CONFIG = {
    'sql_server': {
        'server': 'localhost',
        'database': 'BookHavenDW',
        'username': 'sa',
        'password': 'yourStrong(!)Password'
    },
    'mongodb': {
        'connection_string': 'mongodb://localhost:27017/',
        'database': 'bookhaven_customers'
    }
}
```

Other options: batch size, quality thresholds, cleaning rules, etc.

## ğŸ“Š Data Quality & Monitoring

- **Validation**: Field-level, type, pattern, allowed values, list length, etc. Each rule has severity (ERROR, WARNING, INFO).
- **Cleaning**: Handles dates, emails, phone, numerics, text, duplicates, missing values.
- **Reporting**: Generates detailed data quality reports for each source and stage.

## ğŸ“ Instructions for Students

- Review the data generation scripts to understand the types of data issues introduced.
- Explore the ETL modules (`etl/`) and extend them as needed.
- Add new validation or cleaning rules in `data_quality.py` and `cleaning.py`.
- Use the tests as a starting point for your own test cases.
- Document any changes or extensions you make in the `docs/` folder.

---

**Good luck, and happy ETL-ing at BookHaven!** 