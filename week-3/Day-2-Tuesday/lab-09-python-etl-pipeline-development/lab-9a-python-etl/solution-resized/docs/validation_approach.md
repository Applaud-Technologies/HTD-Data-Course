# Validation Approach - Star Schema ETL Pipeline ‚úÖ

## üéØ Overview

This Star Schema ETL solution implements a **comprehensive, multi-stage validation framework** designed to ensure data quality, business rule compliance, and system integrity throughout the entire pipeline. The approach emphasizes **prevention over correction**, **early detection**, and **continuous quality monitoring**.

## üèóÔ∏è Validation Architecture

### **Multi-Stage Validation Framework**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    INPUT VALIDATION                         ‚îÇ
‚îÇ                 (Configuration & Setup)                     ‚îÇ
‚îÇ           ‚îú‚îÄ‚îÄ Environment variable validation               ‚îÇ
‚îÇ           ‚îú‚îÄ‚îÄ Database connectivity verification            ‚îÇ
‚îÇ           ‚îú‚îÄ‚îÄ File existence and accessibility              ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ Configuration parameter validation            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   EXTRACTION VALIDATION                     ‚îÇ
‚îÇ                 (Data Source Quality)                       ‚îÇ
‚îÇ           ‚îú‚îÄ‚îÄ File format and structure validation          ‚îÇ
‚îÇ           ‚îú‚îÄ‚îÄ Required column presence verification         ‚îÇ
‚îÇ           ‚îú‚îÄ‚îÄ Data type consistency checking                ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ Record-level completeness validation          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 TRANSFORMATION VALIDATION                   ‚îÇ
‚îÇ                 (Business Rule Compliance)                  ‚îÇ
‚îÇ           ‚îú‚îÄ‚îÄ Business logic rule enforcement               ‚îÇ
‚îÇ           ‚îú‚îÄ‚îÄ Data format standardization verification      ‚îÇ
‚îÇ           ‚îú‚îÄ‚îÄ Cross-field consistency validation            ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ Derived value accuracy checking               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   LOADING VALIDATION                        ‚îÇ
‚îÇ                 (Database Integrity)                        ‚îÇ
‚îÇ           ‚îú‚îÄ‚îÄ Referential integrity verification            ‚îÇ
‚îÇ           ‚îú‚îÄ‚îÄ Foreign key constraint validation             ‚îÇ
‚îÇ           ‚îú‚îÄ‚îÄ Business key uniqueness checking              ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ Data warehouse consistency validation         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  PIPELINE VALIDATION                        ‚îÇ
‚îÇ                 (End-to-End Quality)                        ‚îÇ
‚îÇ           ‚îú‚îÄ‚îÄ Row count reconciliation                      ‚îÇ
‚îÇ           ‚îú‚îÄ‚îÄ Data quality scoring                          ‚îÇ
‚îÇ           ‚îú‚îÄ‚îÄ Performance metrics validation                ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ Business outcome verification                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìã Validation Categories and Strategies

### **1. Configuration Validation (config.py)**

#### **Purpose**
Ensure that all required settings are properly configured before pipeline execution begins, preventing runtime failures due to misconfiguration.

#### **Validation Rules**
```python
def _validate_config(self):
    """Comprehensive configuration validation"""
    
    # Database Authentication Validation
    if self.db_auth_type.lower() == "sql":
        if not self.db_username or not self.db_password:
            raise ValueError("DB_USER and DB_PASSWORD required for SQL authentication")
    
    # File Path Validation
    for source_name, file_path in self.data_sources.items():
        if source_name != "baseline_sales":  # Skip SQL table sources
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"Data source file not found: {file_path}")
    
    # Date Range Validation
    try:
        start_date = datetime.strptime(self.date_range_start, "%Y-%m-%d")
        end_date = datetime.strptime(self.date_range_end, "%Y-%m-%d")
        if start_date >= end_date:
            raise ValueError("date_range_start must be before date_range_end")
    except ValueError as e:
        if "time data" in str(e):
            raise ValueError(f"Invalid date format. Use YYYY-MM-DD format: {e}")
        else:
            raise
    
    # Batch Size Validation
    if self.batch_size <= 0 or self.batch_size > 10000:
        raise ValueError("batch_size must be between 1 and 10000")
    
    # Log Directory Creation and Validation
    log_dir = os.path.dirname(self.log_file)
    if log_dir and not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)
```

#### **Validation Categories**
- **Completeness**: All required settings present
- **Format**: Proper data types and formats
- **Logic**: Logical relationships (start date < end date)
- **Accessibility**: Files and directories accessible
- **Ranges**: Values within acceptable bounds

### **2. Data Extraction Validation (extract.py)**

#### **Purpose**
Verify data quality and structure at the point of extraction, catching issues early in the pipeline before they propagate.

#### **File Structure Validation**
```python
def extract_customers_csv(self, file_path: Optional[str] = None) -> List[Dict[str, Any]]:
    """Extract with comprehensive file and data validation"""
    
    # File Existence Validation
    if not file_path or not os.path.exists(file_path):
        raise FileNotFoundError(f"Customer CSV file not found: {file_path}")
    
    # CSV Format Detection and Validation
    with open(file_path, "r", encoding="utf-8", newline="") as csvfile:
        sample = csvfile.read(1024)
        csvfile.seek(0)
        
        try:
            dialect = csv.Sniffer().sniff(sample)
        except csv.Error:
            dialect = csv.excel  # Fallback to standard format
        
        reader = csv.DictReader(csvfile, dialect=dialect)
        
        # Required Column Validation
        required_columns = ["customer_id", "first_name", "last_name", "email"]
        if not all(col in reader.fieldnames for col in required_columns):
            missing_cols = [col for col in required_columns if col not in reader.fieldnames]
            raise ValueError(f"Missing required columns in CSV: {missing_cols}")
        
        # Record-Level Validation
        for row_num, row in enumerate(reader, start=2):
            if self._validate_customer_record(row):
                # Process valid record
                customers.append(clean_row)
            else:
                self.logger.warning(f"Invalid customer record at row {row_num}")
```

#### **Record-Level Validation**
```python
def _validate_customer_record(self, customer: Dict[str, Any]) -> bool:
    """Validate individual customer records against business rules"""
    
    # Required Field Validation
    required_fields = ["customer_id", "first_name", "last_name", "email"]
    for field in required_fields:
        if field not in customer or not customer[field]:
            return False
    
    # Customer ID Format Validation
    customer_id = customer["customer_id"].strip()
    if len(customer_id) < 1 or len(customer_id) > 50:
        return False
    
    # Email Format Validation
    email = customer["email"].strip()
    if "@" not in email or "." not in email.split("@")[-1]:
        return False
    
    # Name Validation
    first_name = customer["first_name"].strip()
    last_name = customer["last_name"].strip()
    if not first_name or not last_name:
        return False
    
    return True

def _validate_product_record(self, product: Dict[str, Any]) -> bool:
    """Validate individual product records"""
    
    # Required Fields
    required_fields = ["product_id", "product_name"]
    for field in required_fields:
        if field not in product or not product[field]:
            return False
    
    # Product ID Validation
    product_id = str(product["product_id"]).strip()
    if len(product_id) < 1 or len(product_id) > 50:
        return False
    
    # Product Name Validation
    name = str(product["product_name"]).strip()
    if not name or len(name) > 200:
        return False
    
    # Price Range Validation
    if "price" in product:
        try:
            price = float(product["price"])
            if price < 0 or price > 100000:  # Reasonable price range
                return False
        except (ValueError, TypeError):
            return False
    
    return True

def _validate_transaction_record(self, transaction: Dict[str, Any]) -> bool:
    """Validate individual transaction records"""
    
    # Required Fields
    required_fields = ["transaction_id", "customer_id", "product_id", "transaction_date"]
    for field in required_fields:
        if field not in transaction or not transaction[field]:
            return False
    
    # Transaction ID Validation
    transaction_id = str(transaction["transaction_id"]).strip()
    if len(transaction_id) < 1 or len(transaction_id) > 50:
        return False
    
    # Quantity Validation
    if "quantity" in transaction:
        try:
            quantity = int(transaction["quantity"])
            if quantity <= 0:
                return False
        except (ValueError, TypeError):
            return False
    
    return True
```

#### **Validation Categories**
- **Structural**: File format, encoding, column presence
- **Completeness**: Required fields present and non-empty
- **Format**: Data types, string lengths, numeric ranges
- **Business Logic**: Email formats, positive quantities, reasonable values

### **3. Data Transformation Validation (transform.py)**

#### **Purpose**
Ensure that business rules are properly applied and data transformations maintain quality and consistency.

#### **Business Rule Validation**
```python
def prepare_customer_dimension(self, customers: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Transform with comprehensive business rule validation"""
    
    transformed_customers = []
    
    for customer in customers:
        try:
            # Name Cleaning and Validation
            first_name, last_name, full_name = self._clean_customer_name(
                customer.get("first_name", ""), 
                customer.get("last_name", "")
            )
            
            # Email Validation and Normalization
            email = customer.get("email", "").strip().lower()
            email_valid = self._validate_email(email)
            if not email_valid:
                self.logger.warning(f"Invalid email for customer {customer.get('customer_id')}: {email}")
                email = ""  # Clear invalid email but continue processing
            
            # Phone Number Validation
            phone = self._standardize_phone(customer.get("phone", ""))
            
            # Customer Segment Validation
            segment = customer.get("segment", "").strip().title()
            if not segment:
                segment = self._classify_customer_segment(customer)
            
            # Build validated transformed record
            transformed_customer = {
                "customer_id": customer["customer_id"],
                "first_name": first_name,
                "last_name": last_name,
                "full_name": full_name,
                "email": email,
                "phone": phone,
                "segment": segment
            }
            
            transformed_customers.append(transformed_customer)
            
        except Exception as e:
            self.logger.warning(f"Error transforming customer {customer.get('customer_id')}: {e}")
            continue
    
    return transformed_customers
```

#### **Data Format Validation**
```python
def _validate_email(self, email: str) -> bool:
    """Email format validation using regex"""
    if not email or not email.strip():
        return False
    return bool(self.email_pattern.match(email.strip().lower()))

def _standardize_phone(self, phone: str) -> str:
    """Phone number validation and standardization"""
    if not phone:
        return ""
    
    # Extract digits only
    digits = self.phone_pattern.sub("", phone.strip())
    
    # Validate length and format
    if len(digits) == 10:
        return f"({digits[:3]}) {digits[3:6]}-{digits[6:]}"
    elif len(digits) == 11 and digits.startswith("1"):
        return f"({digits[1:4]}) {digits[4:7]}-{digits[7:]}"
    elif len(digits) == 7:
        return f"XXX-{digits[:3]}-{digits[3:]}"
    else:
        return phone.strip()  # Return original if invalid

def _determine_price_tier(self, price: float) -> str:
    """Price tier validation and assignment"""
    if price >= 200:
        return "High"
    elif price >= 50:
        return "Medium"
    else:
        return "Low"
```

#### **Cross-Field Validation**
```python
def prepare_sales_facts(self, transactions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """Fact preparation with cross-field validation"""
    
    transformed_facts = []
    
    for transaction in transactions:
        try:
            # Extract and validate basic values
            quantity = int(transaction.get("quantity", 0))
            unit_price = float(transaction.get("unit_price", 0))
            discount_amount = float(transaction.get("discount_amount", 0))
            
            # Calculate derived measures
            gross_amount = quantity * unit_price
            net_amount = gross_amount - discount_amount
            profit_amount = net_amount * 0.20
            
            # Cross-Field Business Rule Validation
            if quantity <= 0 or unit_price <= 0 or net_amount < 0:
                self.logger.warning(f"Invalid transaction amounts for {transaction.get('transaction_id')}")
                continue
            
            # Logical Relationship Validation
            if discount_amount > gross_amount:
                self.logger.warning(f"Discount exceeds gross amount for {transaction.get('transaction_id')}")
                continue
            
            # Date Key Validation
            date_key = self._generate_date_key(transaction.get("transaction_date"))
            if not (10000101 <= date_key <= 99991231):
                self.logger.warning(f"Invalid date key generated: {date_key}")
                continue
            
            # Build validated fact record
            fact_record = {
                "transaction_id": transaction["transaction_id"],
                "customer_id": transaction["customer_id"],
                "product_id": transaction["product_id"],
                "sales_rep_id": transaction.get("sales_rep_id", ""),
                "date_key": date_key,
                "quantity": quantity,
                "unit_price": unit_price,
                "discount_amount": discount_amount,
                "gross_amount": gross_amount,
                "net_amount": net_amount,
                "profit_amount": profit_amount
            }
            
            transformed_facts.append(fact_record)
            
        except Exception as e:
            self.logger.warning(f"Error transforming transaction {transaction.get('transaction_id')}: {e}")
            continue
    
    return transformed_facts
```

#### **Validation Categories**
- **Business Rules**: Domain-specific logic (price tiers, customer segments)
- **Data Quality**: Format standardization, null handling
- **Logical Consistency**: Cross-field relationships, derived value accuracy
- **Range Validation**: Acceptable value ranges, date validity

### **4. Database Loading Validation (load_dimensions.py & load_facts.py)**

#### **Purpose**
Ensure data warehouse integrity through referential constraint validation and business key uniqueness checks.

#### **Dimension Loading Validation**
```python
def load_customer_dimension(self, customers: List[Dict[str, Any]]) -> Dict[str, int]:
    """Dimension loading with comprehensive validation"""
    
    # Business Key Uniqueness Validation
    customer_ids = [c["customer_id"] for c in customers]
    if len(customer_ids) != len(set(customer_ids)):
        duplicates = [id for id in customer_ids if customer_ids.count(id) > 1]
        self.logger.warning(f"Duplicate customer IDs found: {set(duplicates)}")
    
    # Get existing records for SCD validation
    existing_keys = self._get_existing_customer_keys()
    
    # Separate new vs existing customers
    new_customers = []
    existing_customers = []
    
    for customer in customers:
        customer_id = customer["customer_id"]
        if customer_id in existing_keys:
            existing_customers.append(customer)
        else:
            new_customers.append(customer)
    
    # Insert new customers with validation
    new_key_mappings = {}
    if new_customers:
        new_mappings = self._insert_new_customers(new_customers)
        new_key_mappings.update(new_mappings)
    
    # Update existing customers (SCD Type 1) with validation
    if existing_customers:
        self._update_existing_customers(existing_customers, existing_keys)
    
    # Validate final key mappings
    all_key_mappings = {**existing_keys, **new_key_mappings}
    
    return all_key_mappings
```

#### **Fact Loading Validation**
```python
def _validate_fact_record(self, fact_record: Dict[str, Any]) -> bool:
    """Comprehensive fact record validation"""
    
    try:
        # Required Field Validation
        required_fields = ["transaction_id", "customer_id", "product_id", "quantity", "unit_price", "date_key"]
        for field in required_fields:
            if field not in fact_record or fact_record[field] is None:
                return False
        
        # Business Logic Validation
        if fact_record["quantity"] <= 0:
            self.logger.warning(f"Invalid quantity: {fact_record['quantity']}")
            return False
        
        if fact_record["unit_price"] <= 0:
            self.logger.warning(f"Invalid unit price: {fact_record['unit_price']}")
            return False
        
        if fact_record.get("net_amount", 0) < 0:
            self.logger.warning(f"Invalid net amount: {fact_record.get('net_amount')}")
            return False
        
        # Date Key Format Validation
        date_key = fact_record.get("date_key", 0)
        if not (10000101 <= date_key <= 99991231):
            self.logger.warning(f"Invalid date key: {date_key}")
            return False
        
        # Amount Relationship Validation
        gross_amount = fact_record.get("gross_amount", 0)
        net_amount = fact_record.get("net_amount", 0)
        discount_amount = fact_record.get("discount_amount", 0)
        
        expected_gross = fact_record["quantity"] * fact_record["unit_price"]
        if abs(gross_amount - expected_gross) > 0.01:
            self.logger.warning(f"Gross amount calculation error: expected {expected_gross}, got {gross_amount}")
            return False
        
        expected_net = gross_amount - discount_amount
        if abs(net_amount - expected_net) > 0.01:
            self.logger.warning(f"Net amount calculation error: expected {expected_net}, got {net_amount}")
            return False
        
        return True
        
    except (ValueError, TypeError) as e:
        self.logger.warning(f"Validation error for record {fact_record.get('transaction_id')}: {e}")
        return False
```

#### **Referential Integrity Validation**
```python
def _lookup_dimension_keys(self, transaction: Dict[str, Any], dimension_keys: Dict[str, Dict[str, int]]) -> Optional[Dict[str, Any]]:
    """Validate that all foreign key references exist"""
    
    # Get dimension key mappings
    customer_keys = dimension_keys.get("customer_keys", {})
    product_keys = dimension_keys.get("product_keys", {})
    sales_rep_keys = dimension_keys.get("sales_rep_keys", {})
    
    # Lookup each dimension key with validation
    customer_key = customer_keys.get(transaction["customer_id"])
    if not customer_key:
        self.logger.warning(f"Customer key not found for customer_id: {transaction['customer_id']}")
        return None
    
    product_key = product_keys.get(transaction["product_id"])
    if not product_key:
        self.logger.warning(f"Product key not found for product_id: {transaction['product_id']}")
        return None
    
    sales_rep_key = sales_rep_keys.get(transaction.get("sales_rep_id"))
    if not sales_rep_key and transaction.get("sales_rep_id"):
        self.logger.warning(f"Sales rep key not found for sales_rep_id: {transaction['sales_rep_id']}")
        return None
    
    # Date key validation
    date_key = transaction["date_key"]
    if not self._validate_date_key_exists(date_key):
        self.logger.warning(f"Date key does not exist in date dimension: {date_key}")
        return None
    
    return {
        "customer_key": customer_key,
        "product_key": product_key,
        "sales_rep_key": sales_rep_key or 1,  # Default to unknown rep
        "date_key": date_key
    }

def _validate_date_key_exists(self, date_key: int) -> bool:
    """Validate that date key exists in date dimension"""
    try:
        check_sql = "SELECT COUNT(*) FROM dim_date WHERE date_key = ?"
        self.cursor.execute(check_sql, (date_key,))
        count = self.cursor.fetchone()[0]
        return count > 0
    except Exception as e:
        self.logger.error(f"Error validating date key {date_key}: {e}")
        return False
```

#### **Validation Categories**
- **Referential Integrity**: All foreign keys must exist in parent tables
- **Business Key Uniqueness**: No duplicate business keys within dimension
- **Data Type Consistency**: Proper data types for all fields
- **Constraint Compliance**: Database constraint validation

### **5. Pipeline-Level Validation (main.py)**

#### **Purpose**
Provide end-to-end validation of the complete pipeline execution and data quality assessment.

#### **End-to-End Validation**
```python
def _run_validation_phase(self) -> Dict[str, Any]:
    """Comprehensive pipeline validation"""
    
    self.logger.info("Starting data validation and quality checks...")
    
    validation_results = {
        "row_counts": {},
        "referential_integrity": True,
        "data_quality_score": 100.0,
        "validation_errors": []
    }
    
    try:
        connection_string = self.config.get_connection_string()
        
        with pyodbc.connect(connection_string) as conn:
            cursor = conn.cursor()
            
            # Row Count Validation
            tables = ["dim_customer", "dim_product", "dim_date", "dim_sales_rep", "fact_sales"]
            
            for table in tables:
                try:
                    cursor.execute(f"SELECT COUNT(*) FROM {table}")
                    count = cursor.fetchone()[0]
                    validation_results["row_counts"][table] = count
                    self.logger.info(f"  - {table}: {count} rows")
                except Exception as e:
                    error_msg = f"Error counting rows in {table}: {str(e)}"
                    validation_results["validation_errors"].append(error_msg)
                    self.logger.warning(error_msg)
            
            # Referential Integrity Validation
            fact_count = validation_results["row_counts"].get("fact_sales", 0)
            
            if fact_count > 0:
                try:
                    # Validate all foreign key relationships
                    integrity_sql = """
                    SELECT COUNT(*) FROM fact_sales f
                    WHERE EXISTS (SELECT 1 FROM dim_customer c WHERE c.customer_key = f.customer_key)
                      AND EXISTS (SELECT 1 FROM dim_product p WHERE p.product_key = f.product_key)
                      AND EXISTS (SELECT 1 FROM dim_date d WHERE d.date_key = f.date_key)
                      AND EXISTS (SELECT 1 FROM dim_sales_rep s WHERE s.sales_rep_key = f.sales_rep_key)
                    """
                    
                    cursor.execute(integrity_sql)
                    valid_fact_count = cursor.fetchone()[0]
                    
                    if valid_fact_count == fact_count:
                        self.logger.info("  - All foreign key relationships are valid")
                    else:
                        validation_results["referential_integrity"] = False
                        orphan_count = fact_count - valid_fact_count
                        error_msg = f"Found {orphan_count} fact records with invalid foreign keys"
                        validation_results["validation_errors"].append(error_msg)
                        self.logger.warning(error_msg)
                
                except Exception as e:
                    error_msg = f"Error checking referential integrity: {str(e)}"
                    validation_results["validation_errors"].append(error_msg)
                    self.logger.warning(error_msg)
            
            # Data Quality Score Calculation
            total_records = sum(validation_results["row_counts"].values())
            if total_records > 0 and validation_results["referential_integrity"]:
                validation_results["data_quality_score"] = 100.0
            elif total_records > 0:
                validation_results["data_quality_score"] = 75.0
            else:
                validation_results["data_quality_score"] = 0.0
            
            # Business Logic Validation
            if fact_count > 0:
                # Validate no negative amounts
                cursor.execute("SELECT COUNT(*) FROM fact_sales WHERE net_amount < 0")
                negative_amounts = cursor.fetchone()[0]
                
                if negative_amounts > 0:
                    error_msg = f"Found {negative_amounts} fact records with negative amounts"
                    validation_results["validation_errors"].append(error_msg)
                    validation_results["data_quality_score"] *= 0.9
                
                # Validate reasonable quantities
                cursor.execute("SELECT COUNT(*) FROM fact_sales WHERE quantity <= 0 OR quantity > 1000")
                invalid_quantities = cursor.fetchone()[0]
                
                if invalid_quantities > 0:
                    error_msg = f"Found {invalid_quantities} fact records with invalid quantities"
                    validation_results["validation_errors"].append(error_msg)
                    validation_results["data_quality_score"] *= 0.95
    
    except Exception as e:
        error_msg = f"Validation phase failed: {str(e)}"
        self.logger.error(error_msg)
        validation_results["validation_errors"].append(error_msg)
        validation_results["data_quality_score"] = 0.0
    
    return validation_results
```

#### **Data Quality Scoring**
```python
def _calculate_data_quality_score(self, validation_results: Dict[str, Any]) -> float:
    """Calculate overall data quality score based on validation results"""
    
    base_score = 100.0
    
    # Penalize for referential integrity issues
    if not validation_results.get("referential_integrity", True):
        base_score *= 0.7
    
    # Penalize for validation errors
    error_count = len(validation_results.get("validation_errors", []))
    if error_count > 0:
        penalty = min(0.5, error_count * 0.1)  # Max 50% penalty
        base_score *= (1.0 - penalty)
    
    # Penalize for missing data
    row_counts = validation_results.get("row_counts", {})
    expected_tables = ["dim_customer", "dim_product", "fact_sales"]
    missing_data = sum(1 for table in expected_tables if row_counts.get(table, 0) == 0)
    
    if missing_data > 0:
        base_score *= (1.0 - (missing_data / len(expected_tables)) * 0.5)
    
    return max(0.0, min(100.0, base_score))
```

## üéØ Validation Metrics and Reporting

### **Quality Metrics Collection**
```python
validation_metrics = {
    # Completeness Metrics
    "record_counts": {
        "extracted": 15420,
        "transformed": 15398,
        "loaded": 15385,
        "success_rate": 99.77
    },
    
    # Accuracy Metrics
    "validation_results": {
        "email_format_errors": 12,
        "phone_format_errors": 8,
        "business_rule_violations": 5,
        "accuracy_rate": 99.84
    },
    
    # Integrity Metrics
    "referential_integrity": {
        "orphaned_facts": 0,
        "invalid_foreign_keys": 0,
        "constraint_violations": 0,
        "integrity_score": 100.0
    },
    
    # Consistency Metrics
    "data_consistency": {
        "duplicate_business_keys": 0,
        "cross_field_inconsistencies": 2,
        "format_standardization_rate": 99.99
    }
}
```

### **Validation Reporting**
```python
def generate_validation_report(self, validation_results: Dict[str, Any]) -> str:
    """Generate comprehensive validation report"""
    
    report = []
    report.append("=" * 60)
    report.append("DATA VALIDATION REPORT")
    report.append("=" * 60)
    
    # Overall Quality Score
    quality_score = validation_results.get("data_quality_score", 0)
    report.append(f"Overall Data Quality Score: {quality_score:.1f}%")
    
    # Row Counts
    report.append("\nRow Counts:")
    for table, count in validation_results.get("row_counts", {}).items():
        report.append(f"  - {table}: {count:,} records")
    
    # Referential Integrity
    integrity_status = "PASS" if validation_results.get("referential_integrity", False) else "FAIL"
    report.append(f"\nReferential Integrity: {integrity_status}")
    
    # Validation Errors
    errors = validation_results.get("validation_errors", [])
    if errors:
        report.append(f"\nValidation Issues Found: {len(errors)}")
        for error in errors[:10]:  # Show first 10 errors
            report.append(f"  - {error}")
        if len(errors) > 10:
            report.append(f"  ... and {len(errors) - 10} more issues")
    else:
        report.append("\nNo validation issues found.")
    
    report.append("=" * 60)
    
    return "\n".join(report)
```

## üèÜ Validation Benefits

### **Data Quality Assurance**
- **Early Detection**: Issues caught at extraction prevent downstream problems
- **Business Rule Enforcement**: Consistent application of business logic
- **Format Standardization**: Uniform data formats across the warehouse
- **Referential Integrity**: Guaranteed foreign key relationships

### **Operational Benefits**
- **Reduced Manual Review**: Automated validation reduces manual data checking
- **Faster Issue Resolution**: Detailed validation reports accelerate debugging
- **Quality Monitoring**: Continuous tracking of data quality trends
- **Compliance Support**: Documented validation for regulatory requirements

### **Business Benefits**
- **Trusted Analytics**: High-quality data supports confident business decisions
- **Reduced Errors**: Validation prevents incorrect business insights
- **Improved Efficiency**: Clean data reduces time spent on data cleaning
- **Risk Mitigation**: Early detection prevents costly data quality issues

## üîß Best Practices Demonstrated

### **1. Fail Fast Validation**
- **Configuration validation** stops pipeline before processing begins
- **File structure validation** catches issues early in extraction
- **Required field validation** prevents incomplete data processing
- **Format validation** ensures data consistency from the start

### **2. Layered Validation Strategy**
- **Input validation**: Verify configuration and data sources
- **Process validation**: Check business rules during transformation
- **Output validation**: Ensure database integrity after loading
- **End-to-end validation**: Verify complete pipeline success

### **3. Graceful Degradation**
- **Continue processing** despite individual record failures
- **Log validation issues** without stopping pipeline execution
- **Provide partial results** when some data sources fail
- **Track success rates** for quality monitoring

### **4. Comprehensive Error Context**
- **Specific error messages** with actionable guidance
- **Row-level error tracking** for precise debugging
- **Business context** in validation messages
- **Error categorization** for systematic resolution

### **5. Data Quality Scoring**
- **Quantitative quality metrics** for trend analysis
- **Multi-dimensional scoring** (completeness, accuracy, integrity)
- **Continuous monitoring** of validation results
- **Quality threshold management** for automated alerting

### **6. Business Rule Enforcement**
- **Domain-specific validation** for each data type
- **Cross-field consistency** checking
- **Logical relationship** validation
- **Industry standard** compliance verification

## üåü Production Impact

### **Data Reliability**
This validation framework ensures that:
- **Analytics are trustworthy** with high-quality validated data
- **Business decisions** are based on accurate, consistent information
- **Regulatory compliance** is maintained through documented validation
- **Data lineage** is clear with comprehensive audit trails

### **Operational Excellence**
The validation approach provides:
- **Automated quality control** reducing manual review overhead
- **Early problem detection** preventing downstream data issues
- **Systematic error resolution** with detailed diagnostic information
- **Continuous improvement** through quality trend monitoring

### **Business Value**
Organizations benefit from:
- **Reduced data quality incidents** through proactive validation
- **Faster time-to-insight** with pre-validated analytical data
- **Improved customer experience** through accurate data-driven decisions
- **Lower operational costs** via automated quality management

## üìã Validation Checklist

### **Implementation Checklist**
- ‚úÖ **Configuration validation** implemented and tested
- ‚úÖ **File structure validation** handles all expected formats
- ‚úÖ **Record-level validation** covers all business rules
- ‚úÖ **Transformation validation** ensures data quality
- ‚úÖ **Database integrity validation** verifies referential constraints
- ‚úÖ **End-to-end validation** provides complete quality assessment
- ‚úÖ **Error reporting** generates actionable validation reports
- ‚úÖ **Quality scoring** tracks data quality trends over time

### **Monitoring Checklist**
- ‚úÖ **Validation metrics** collected and stored
- ‚úÖ **Quality dashboards** display real-time validation status
- ‚úÖ **Automated alerts** notify teams of quality issues
- ‚úÖ **Trend analysis** identifies quality improvement opportunities
- ‚úÖ **SLA monitoring** tracks quality against business requirements

## üöÄ Future Enhancements

### **Advanced Validation Techniques**
- **Statistical validation**: Detect anomalies using statistical methods
- **Machine learning validation**: Use ML models to identify data quality issues
- **Real-time validation**: Implement streaming validation for real-time data
- **Cross-system validation**: Validate data consistency across multiple systems

### **Integration Opportunities**
- **Data quality tools**: Integrate with specialized DQ platforms
- **Monitoring systems**: Connect to enterprise monitoring infrastructure
- **Alerting platforms**: Integrate with incident management systems
- **Governance tools**: Connect to data governance and lineage platforms

This comprehensive validation approach makes the Star Schema ETL pipeline **enterprise-ready** by ensuring data quality, business rule compliance, and operational reliability throughout the entire data processing lifecycle.
